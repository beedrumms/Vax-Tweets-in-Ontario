{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Preprocessing Vax Tweets.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "mount_file_id": "17D3zmb4T-MsYjrinwv5DlB38oltTHEVc",
      "authorship_tag": "ABX9TyPojZQ030ySOw+sLHk91Pc9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/beedrumms/Vax-Tweets-in-Ontario/blob/main/Preprocessing_Vax_Tweets.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yYVe7f9ejBYw"
      },
      "source": [
        "# import essential packages for basic processing\n",
        "import re \n",
        "import string\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from wordcloud import WordCloud\n",
        "from datetime import datetime\n",
        "\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import wordnet as wn\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "\n",
        "import gensim\n",
        "from gensim.parsing.preprocessing import remove_stopwords\n",
        "\n",
        "\n",
        "import sklearn \n",
        "from sklearn import model_selection, naive_bayes, svm\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "\n",
        "from collections import defaultdict\n",
        "from pprint import pprint\n",
        "\n",
        "from google.colab import drive, files\n",
        "import os\n",
        "\n",
        "# Mounting google colab on drive \n",
        "drive.mount(\"/content/drive\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EA7zLmb0ZRyP"
      },
      "source": [
        "data = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/vaccine_tweets.csv')\n",
        "tweets = data.iloc[:, [3,10]]\n",
        "tweets.columns = [\"Datetime\", \"Text\"]\n",
        "print(len(tweets['Text']))\n",
        "tweets.drop_duplicates(\"Text\", inplace = True)\n",
        "tweets[:3], print(len(tweets['Text']))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4o0Smrf-B1Xr"
      },
      "source": [
        "#Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dSpO4tuiQE_n"
      },
      "source": [
        "# Cleaning functions for text data\n",
        "import re \n",
        "import string\n",
        "def clean(tweets):\n",
        "    tweets = tweets.lower()\n",
        "    tweets = re.sub('https://[A-Za-z0-9./]+','',tweets) #remove links \n",
        "    tweets = re.sub('https//[A-Za-z0-9./]+','',tweets) #remove broken links that do not have a colon\n",
        "    tweets = re.sub(r'w{3}.\\w+.\\w+','',tweets) # remove other links with www\n",
        "    tweets = re.sub('[\\\\r\\\\n?|\\\\n]','',tweets) # remove carrige returns and new lines\n",
        "    tweets = re.sub('#[a-zA-Z0-9-_]+', '', tweets) # remove all hashtags\n",
        "    tweets = re.sub('\\s[&]amp[;]\\s',' and ',tweets)\n",
        "    tweets = re.sub('@\\w{0,18}[a-zA-Z0-9_]','', tweets) # removing all twitter handles\n",
        "    tweets = re.sub('[#\\'\"%\\-_.?!,;:&/\\\\*\\]]', '',  tweets) # remove all punctuation\n",
        "    tweets = re.sub('[’‘“”]', '', tweets) # removing this weird apostrophe that a regular apostrophe wont get rid of  \n",
        "    tweets = re.sub('[0-9’]', '', tweets) # remove digits \n",
        "    return tweets\n",
        "\n",
        "cleaning = lambda x: clean(x)\n",
        "\n",
        "def removing_emojis(text):\n",
        "  regrex_pattern = re.compile(pattern = \"[\"\n",
        "  u\"\\U0001F600-\\U0001F64F\" \n",
        "        u\"\\U0001F300-\\U0001F5FF\"  \n",
        "        u\"\\U0001F680-\\U0001F6FF\"  \n",
        "        u\"\\U0001F1E0-\\U0001F1FF\"  \n",
        "                           \"]+\", flags = re.UNICODE)\n",
        "  return regrex_pattern.sub(r'',text)\n",
        "\n",
        "emoji_removal = lambda x: removing_emojis(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ndgkVh3CQabJ"
      },
      "source": [
        "tweets_df = tweets.copy() # copying dataframe\n",
        "tweets_for_sentiments = tweets_df['Text'] # copying text data so all datasets will have original and processed tweets \n",
        "tweets_df['Text_Unprocessed'] = tweets_for_sentiments # adding it back \n",
        "tweets_df.Text = pd.DataFrame(tweets_df.Text.apply(cleaning))\n",
        "tweets_df.Text = pd.DataFrame(tweets_df.Text.apply(emoji_removal))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "btSVUNS4eJ2D"
      },
      "source": [
        "# This function leaves caps and some puncuation for better performance in sentiment analyzer\n",
        "def basic_clean(tweets):\n",
        "    tweets = re.sub('https://[A-Za-z0-9./]+','',tweets) #remove links \n",
        "    tweets = re.sub('https//[A-Za-z0-9./]+','',tweets) #remove broken links that do not have a colon\n",
        "    tweets = re.sub(r'w{3}.\\w+.\\w+','',tweets) # remove other links with www\n",
        "    tweets = re.sub('[\\\\r\\\\n?|\\\\n]','',tweets) # remove carrige returns and new lines\n",
        "    tweets = re.sub('#[a-zA-Z0-9-_]+', '', tweets) # remove all hashtags\n",
        "    tweets = re.sub('\\s[&]amp[;]\\s',' and ',tweets)\n",
        "    tweets = re.sub('@\\w{0,18}[a-zA-Z0-9_]','', tweets) # remove remaining usernames\n",
        "    tweets = re.sub('[#\\'\"%\\-_;\\\\*\\]]', '',  tweets) # remove some punctuation\n",
        "    tweets = re.sub('[’“”]', '', tweets) # removing this weird apostrophe that a regular apostrophe wont get rid of  \n",
        "    tweets = re.sub('[0-9’]', '', tweets) # remove digits \n",
        "    return tweets\n",
        "\n",
        "basic_cleaning = lambda x: basic_clean(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W6kUqV02itw_"
      },
      "source": [
        "tweets_df.Text_Unprocessed = pd.DataFrame(tweets_df.Text_Unprocessed.apply(basic_cleaning))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iBCzaQd8Qjvw"
      },
      "source": [
        "# removing stop words\n",
        "tweets_no_stopwords = []\n",
        "for t in tweets_df['Text']:\n",
        "  tweets_no_stopwords.append(remove_stopwords(t))\n",
        "\n",
        "# Removing white spaces\n",
        "removed_spaces = []\n",
        "for t in tweets_no_stopwords:\n",
        "  removed_spaces.append(t.strip())\n",
        "\n",
        "# ensure tweets are a string\n",
        "tweet_string = [] \n",
        "for t in removed_spaces:\n",
        "  tweet_string.append(str(t))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ifbf2chQmP_"
      },
      "source": [
        "# Installing spacy model\n",
        "!pip install -U spacy\n",
        "!python -m spacy validate\n",
        "!python -m spacy download en_core_web_md"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2JwUDHSDQnyv"
      },
      "source": [
        "# Lemmatize \n",
        "import spacy \n",
        "nlp = spacy.load('en_core_web_md', disable=['parser', 'ner'])\n",
        "docs_processed = [list(nlp(t)) for t in tweet_string]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q0NediTgQsjP"
      },
      "source": [
        "# Recreate sentences with lemma forms\n",
        "lemma_form = []\n",
        "lemma_sentences = [] \n",
        "for t in docs_processed:\n",
        "  for w in t: \n",
        "    lemma_form.append(w.lemma_)\n",
        "  lemma_sentences.append(' '.join(lemma_form))\n",
        "  lemma_form = []\n",
        "\n",
        "final_texts = [] \n",
        "for t in lemma_sentences:\n",
        "  final_texts.append(str(t))\n",
        "\n",
        "# Overwrite text data in dataframe with lemma versions of tweets\n",
        "tweets_df['Text'] = final_texts"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FGnun1_4SBFJ"
      },
      "source": [
        "tweets_df.to_csv('/content/drive/MyDrive/Colab Notebooks/processed_vax_tweets.csv')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}